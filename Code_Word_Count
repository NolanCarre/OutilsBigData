#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Dec 10 14:42:46 2020

@author: carrenolan
"""
install_requires=[
    'pyspark=={site.SPARK_VERSION}'
    ]

#On import pyspark 
from pyspark import SparkContext,SparkConf

#Instantiation l'API de spark pour les RDD (spark context).
conf =SparkConf().setAppName("wordCount").setMaster("local")
sc = SparkContext(conf=conf)

#On importe le fichier sample.txt
distFile = sc.textFile('sample.txt')

#on split avec la fonction split() sur les tabulations pour chaque lignes de données pour obtenir une collection de mots.
#avec la foncttion map() pour chaque mot on fait la transformation mot -> Tuple(mot,1)
#On regroupe par la clé "word"  et on fait le comptage
count = distFile.flatMap(lambda line: line.split(" ")).map(lambda word: (word,1)).reduceByKey(lambda a,b :a+b)

#On affiche le contenu de count
print(count.collect())

#On exporte le contenu de count (RDD) dans un fichier au format .txt
count.coalesce(1).saveAsTextFile("LeCompteur")